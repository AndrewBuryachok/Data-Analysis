{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ukr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "  lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "  eng, ukr, _ = line.split(\"\\t\")\n",
    "  ukr = \"[start] \" + ukr + \" [end]\"\n",
    "  text_pairs.append((eng, ukr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I worked yesterday, but Tom didn't.\", '[start] Я вчора працював, а Том ні. [end]')\n",
      "('There are alternatives.', '[start] Альтернативи є. [end]')\n",
      "(\"Aren't you sad?\", '[start] Хіба вам не сумно? [end]')\n",
      "(\"Have you told Tom I'm here?\", '[start] Ви сказали Тому, що я тут? [end]')\n",
      "('Do what I say.', '[start] Роби, що я кажу. [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "  print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158174 total pairs\n",
      "110722 training pairs\n",
      "23726 validation pairs\n",
      "23726 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "  lowercase = tf.strings.lower(input_string)\n",
    "  return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = keras.layers.TextVectorization(\n",
    "  max_tokens=vocab_size,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=sequence_length,\n",
    ")\n",
    "ukr_vectorization = keras.layers.TextVectorization(\n",
    "  max_tokens=vocab_size,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=sequence_length + 1,\n",
    "  standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
    "eng_vectorization.adapt(train_eng_texts)\n",
    "ukr_vectorization.adapt(train_ukr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, ukr):\n",
    "  eng = eng_vectorization(eng)\n",
    "  ukr = ukr_vectorization(ukr)\n",
    "  return ({ \"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "  eng_texts, ukr_texts = zip(*pairs)\n",
    "  eng_texts = list(eng_texts)\n",
    "  ukr_texts = list(ukr_texts)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ukr_texts))\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.map(format_dataset)\n",
    "  return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "  print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "  print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "  print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(keras.layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerEncoderLayer, self).__init__()\n",
    "    self.attention = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.dense_proj = tf.keras.Sequential([keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim)])\n",
    "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = keras.layers.Dropout(rate)\n",
    "    self.dropout2 = keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, training):\n",
    "    attn_output = self.attention(inputs, inputs)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output)\n",
    "    ffn_output = self.dense_proj(out1)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(keras.layers.Layer):\n",
    "  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "    super(TransformerDecoderLayer, self).__init__()\n",
    "    self.attention1 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.attention2 = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    self.dense_proj = tf.keras.Sequential([keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim)])\n",
    "    self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.dropout1 = keras.layers.Dropout(rate)\n",
    "    self.dropout2 = keras.layers.Dropout(rate)\n",
    "    self.dropout3 = keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, inputs, enc_output, training):\n",
    "    attn_output1 = self.attention1(inputs, inputs)\n",
    "    attn_output1 = self.dropout1(attn_output1, training=training)\n",
    "    out1 = self.layernorm1(inputs + attn_output1)\n",
    "    attn_output2 = self.attention2(out1, enc_output)\n",
    "    attn_output2 = self.dropout2(attn_output2, training=training)\n",
    "    out2 = self.layernorm2(out1 + attn_output2)\n",
    "    ffn_output = self.dense_proj(out2)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    return self.layernorm3(out2 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoderLayer(embed_dim, num_heads, latent_dim)(x)\n",
    "\n",
    "decoder_inputs = keras.layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoderLayer(embed_dim, num_heads, latent_dim)(x, encoder_outputs)\n",
    "decoder_outputs = keras.layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "\n",
    "transformer = keras.models.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1731/1731 [==============================] - 2030s 1s/step - loss: 1.6029 - accuracy: 0.7003 - val_loss: 0.8385 - val_accuracy: 0.7115\n",
      "1731/1731 [==============================] - 2025s 1s/step - loss: 1.4293 - accuracy: 0.8208 - val_loss: 0.7367 - val_accuracy: 0.7653\n",
      "1731/1731 [==============================] - 2033s 1s/step - loss: 1.2618 - accuracy: 0.9072 - val_loss: 0.6785 - val_accuracy: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x214839fe920>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: He is always happy.",
      "OUTPUT: [start] Він веселий [end]",
      "INPUT: Nobody saw anything.",
      "OUTPUT: [start] Ніхто бачив нічого [end]",
      "INPUT: Clean the room.",
      "OUTPUT: [start] Чиста кімната [end]",
      "INPUT: Who's in the kitchen?",
      "OUTPUT: [start] Хто в кухні [end]",
      "INPUT: I've come to pick Tom up.",
      "OUTPUT: [start] Я повернувся Том зверху [end]"
     ]
    }
   ],
   "source": [
    "ukr_vocab = ukr_vectorization.get_vocabulary()\n",
    "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "  tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "  decoded_sentence = \"[start]\"\n",
    "  for i in range(max_decoded_sentence_length):\n",
    "    tokenized_target_sentence = ukr_vectorization([decoded_sentence])[:, :-1]\n",
    "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "    sampled_token = ukr_index_lookup[sampled_token_index]\n",
    "    decoded_sentence += \" \" + sampled_token\n",
    "    if sampled_token == \"[end]\":\n",
    "      break\n",
    "  return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(5):\n",
    "  input_sentence = random.choice(test_eng_texts)\n",
    "  translated = decode_sequence(input_sentence)\n",
    "  print(f'INPUT: {input_sentence}')\n",
    "  print(f'OUTPUT: {translated}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
